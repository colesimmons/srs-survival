{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3017762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dependencies ---\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ad914d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data preprocessing ---\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('output.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# Sort dataframe by timestamp\n",
    "df.sort_values(['card_id', 'timestamp'], ascending=True, inplace=True)\n",
    "\n",
    "# Calculate time since last review for each review\n",
    "df['time_since_last_review'] = df.groupby('card_id')['timestamp'].diff().dt.total_seconds().fillna(0) / (60 * 60 * 24)  # convert to days\n",
    "\n",
    "# Calculate time since initial review for each review\n",
    "df['time_since_initial_review'] = (df['timestamp'] - df.groupby('card_id')['timestamp'].transform('first')).dt.total_seconds() / (60 * 60 * 24)  # convert to days\n",
    "\n",
    "# Drop cards with fewer than 4 reviews\n",
    "df = df.groupby('card_id').filter(lambda x: len(x) >= 4)\n",
    "\n",
    "# Normalize time columns\n",
    "scaler = MinMaxScaler()\n",
    "normalized = scaler.fit_transform(df[['time_since_last_review', 'time_since_initial_review']])\n",
    "df[['time_since_last_review', 'time_since_initial_review']] = normalized\n",
    "\n",
    "# Drop 'timestamp'\n",
    "df = df.drop(columns=['timestamp'])\n",
    "\n",
    "unique_card_ids = df['card_id'].unique()\n",
    "    \n",
    "train_card_ids, test_card_ids = train_test_split(unique_card_ids, test_size=0.2, random_state=42)\n",
    "df_train = df[df['card_id'].isin(train_card_ids)]\n",
    "df_test = df[df['card_id'].isin(test_card_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "360b62e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_sequences(df, balance=False):\n",
    "    card_ids = df['card_id'].unique()\n",
    "\n",
    "    sequences = [\n",
    "        (\n",
    "          df[df['card_id'] == card_id].iloc[:i].drop(columns=['card_id', 'was_remembered']).values,\n",
    "          (df[df['card_id'] == card_id].iloc[i]['time_since_last_review'], df[df['card_id'] == card_id].iloc[i]['was_remembered'])\n",
    "        ) for card_id in card_ids for i in range(2, len(df[df['card_id'] == card_id]))\n",
    "    ]\n",
    "    \n",
    "    # shuffle the sequences\n",
    "    random.shuffle(sequences)\n",
    "    \n",
    "    if balance is False:\n",
    "        return sequences\n",
    "    \n",
    "    # separate positive and negative examples\n",
    "    positive = [seq for seq in sequences if seq[1][1] == 1]\n",
    "    negative = [seq for seq in sequences if seq[1][1] == 0]\n",
    "\n",
    "    # undersample positive examples if they outnumber the negative ones\n",
    "    negative += random.choices(negative, k=len(positive) - len(negative))\n",
    "\n",
    "    # combine positive and negative examples and shuffle them again\n",
    "    balanced = positive + negative\n",
    "    random.shuffle(balanced)\n",
    "    return balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51ee3993",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = fmt_sequences(df_train, balance=True)\n",
    "test_sequences = fmt_sequences(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26f85e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [sequence for sequence, target in train_sequences]\n",
    "X_test = [sequence for sequence, target in test_sequences]\n",
    "Y_train = [target for sequence, target in train_sequences]\n",
    "Y_test = [target for sequence, target in test_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8c0d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Datasets ---\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx]).float(), torch.tensor(self.y[idx]).float()\n",
    "\n",
    "# Pad the sequences and create your DataLoader\n",
    "def collate_fn(batch):\n",
    "    inputs = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    \n",
    "    # Pad sequences so they all have the same length within one batch\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "\n",
    "    return inputs_padded, torch.stack(targets)\n",
    "\n",
    "train_dataset = ReviewDataset(X_train, Y_train)\n",
    "test_dataset = ReviewDataset(X_test, Y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9040e075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0896eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model ---\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Define output layer\n",
    "        self.fc = nn.Linear(hidden_size, 2)  # Two output nodes for λ and k\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden states: (num_layers, batch_size, hidden_size)\n",
    "        #h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) \n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        y, hidden = self.lstm(x)  # shape = (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(y[:, -1, :])  # only take the output from the last time step\n",
    "        # Get parameters of the Weibull distribution\n",
    "        λ, k = torch.exp(out[:, 0]), torch.exp(out[:, 1])  # Ensure positive values\n",
    "        return λ, k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32685941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_survival_function(λ, k, t):\n",
    "    return torch.exp(- (t / λ) ** k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c96688ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_size=3, hidden_size=64, num_layers=2)  # Adjust the parameters as needed\n",
    "# model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adjust the learning rate as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2e40198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.7778677344322205, Train F1: 0.23391595388620307\n",
      "Test F1: 0.2392694063926941\n",
      "Epoch 2/10, Loss: 0.49228233098983765, Train F1: 0.09665613042754949\n",
      "Test F1: 0.09115103127079174\n",
      "Epoch 3/10, Loss: 0.9704979062080383, Train F1: 0.2138215732742145\n",
      "Test F1: 0.22002166176698126\n",
      "Epoch 4/10, Loss: 0.301880806684494, Train F1: 0.2646669607410675\n",
      "Test F1: 0.2724143092351444\n",
      "Epoch 5/10, Loss: 2.3274905681610107, Train F1: 0.3108874397504962\n",
      "Test F1: 0.31198015467678386\n",
      "Epoch 6/10, Loss: 0.7980320453643799, Train F1: 0.20646312604259612\n",
      "Test F1: 0.20062597809076685\n",
      "Epoch 7/10, Loss: 0.5038732886314392, Train F1: 0.29646803115377646\n",
      "Test F1: 0.2941870758335792\n",
      "Epoch 8/10, Loss: 0.5049183368682861, Train F1: 0.21614926066424092\n",
      "Test F1: 0.21016632986165087\n",
      "Epoch 9/10, Loss: 1.3608896732330322, Train F1: 0.16186324372325825\n",
      "Test F1: 0.15196394075981973\n",
      "Epoch 10/10, Loss: 0.30295881628990173, Train F1: 0.2536574301591502\n",
      "Test F1: 0.24639222239100714\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        λ, k = model(inputs)\n",
    "        review_time = targets[:, 0]\n",
    "        remembered = targets[:, 1]\n",
    "        \n",
    "        # Compute predicted survival probabilities at the time of the next review\n",
    "        predicted_survival = weibull_survival_function(λ, k, review_time)\n",
    "\n",
    "        # Compute the binary cross-entropy loss between the predicted probabilities and the true outcomes\n",
    "        loss = nn.BCELoss()(predicted_survival, remembered)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in train_loader:\n",
    "            λ, k = model(inputs)\n",
    "            review_time = targets[:, 0]\n",
    "            remembered = targets[:, 1]\n",
    "            predicted_survival = weibull_survival_function(λ, k, review_time)\n",
    "            preds = (predicted_survival > 0.5).float()\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(remembered)\n",
    "    train_f1 = f1_score(all_targets, all_preds)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Train F1: {train_f1}')\n",
    "    \n",
    "    # Test\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            λ, k = model(inputs)\n",
    "            review_time = targets[:, 0]\n",
    "            remembered = targets[:, 1]\n",
    "            predicted_survival = weibull_survival_function(λ, k, review_time)\n",
    "            preds = (predicted_survival > 0.5).float()\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(remembered)\n",
    "    test_f1 = f1_score(all_targets, all_preds)\n",
    "\n",
    "    print(f'Test F1: {test_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f95b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
